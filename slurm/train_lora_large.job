#!/bin/bash
#SBATCH --job-name=lora_large
#SBATCH --gres=gpu:1
#SBATCH --constraint="gpu_3090|gpu_a5000|gpu_5000_ada"
#SBATCH --cpus-per-task=12
#SBATCH --mem=64G
#SBATCH --time=12:00:00
#SBATCH --output=/mnt/fast/nobackup/users/gb0048/opro2/logs/train_lora_large_%j.out
#SBATCH --error=/mnt/fast/nobackup/users/gb0048/opro2/logs/train_lora_large_%j.err

# =============================================================================
# PURPOSE: Train Qwen2-Audio with LoRA on LARGE dataset (Block F)
# =============================================================================
# Uses experimental_variants_large (3x more data than standard):
#   - Train: 9,600 samples (vs 3,072)
#   - Dev: 4,800 samples (vs 3,456)
#
# INPUTS:
#   data/processed/experimental_variants_large/{train,dev}_metadata.csv
#
# OUTPUT:
#   checkpoints/qwen_lora_large_seed42/
# =============================================================================

set -euo pipefail
set -x

REPO="/mnt/fast/nobackup/users/gb0048/opro2"
CONTAINER="$REPO/qwen_pipeline_v2.sif"

echo "[INFO] Start: $(date)"
echo "[INFO] Allocated GPU:"
nvidia-smi --query-gpu=name,memory.total --format=csv
cd "$REPO"

export HF_HOME="/mnt/fast/nobackup/users/gb0048/.cache/huggingface"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_HUB_CACHE="$HF_HOME/hub"
mkdir -p "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"

# Fix CUDA OOM: Enable expandable memory segments
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Verify data exists
TRAIN_CSV="$REPO/data/processed/experimental_variants_large/train_metadata.csv"
VAL_CSV="$REPO/data/processed/experimental_variants_large/dev_metadata.csv"

if [ ! -f "$TRAIN_CSV" ]; then
    echo "[ERROR] Training data not found: $TRAIN_CSV"
    echo "[ERROR] Run 'sbatch slurm/prepare_data_large.job' first"
    exit 1
fi

echo "[INFO] Training samples: $(wc -l < "$TRAIN_CSV")"
echo "[INFO] Validation samples: $(wc -l < "$VAL_CSV")"

echo "[RUN] Training Qwen2-Audio with LoRA on LARGE dataset (seed 42)"
apptainer exec --nv \
  --env HF_HOME="$HF_HOME" \
  --env TRANSFORMERS_CACHE="$TRANSFORMERS_CACHE" \
  --env HF_HUB_CACHE="$HF_HUB_CACHE" \
  --env PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  "$CONTAINER" python3 scripts/finetune_qwen_audio.py \
  --train_csv "$TRAIN_CSV" \
  --val_csv "$VAL_CSV" \
  --output_dir checkpoints/qwen_lora_large_seed42 \
  --seed 42 \
  --num_epochs 3 \
  --learning_rate 5e-5 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 4

echo "[DONE] End: $(date)"
echo "Checkpoint: checkpoints/qwen_lora_large_seed42/"
