# Setup para RTX 4070 Laptop (8GB VRAM)

**Tu Hardware**: RTX 4070 Laptop, 8GB VRAM
**Estrategia**: Carga/descarga alternada de modelos

---

## üéØ Estrategia Optimizada

Tu GPU tiene **8GB VRAM**, que es suficiente para correr UN modelo a la vez pero no DOS simult√°neamente.

**Soluci√≥n**: Script optimizado que alterna entre modelos:

```
Iteraci√≥n N:
  1. Carga Llama 3.2-3B (generador) ‚îÄ‚îÄ> Genera 3 prompts ‚îÄ‚îÄ> Descarga
  2. Carga Qwen2-Audio (evaluador) ‚îÄ‚îÄ> Eval√∫a 3 prompts ‚îÄ‚îÄ> Descarga
  3. Repite
```

**Ventajas**:
- ‚úÖ Funciona perfectamente con 8GB
- ‚úÖ No necesita API keys (100% local)
- ‚úÖ Usa Llama 3.2 (heterogeneidad como pediste)

**Desventaja**:
- ‚è±Ô∏è M√°s lento (~50-60 min por iteraci√≥n vs 40 min con 2 modelos simult√°neos)

---

## üöÄ Comando para Prototipar (5 iteraciones)

```bash
# Test en tu laptop (3-4 horas)
python scripts/run_opro_local_8gb.py \
    --n_iterations 5 \
    --early_stopping 3 \
    --output_dir results/sprint9_opro_laptop_test
```

**Tiempo estimado**: 3-4 horas (perfecto para prototipar)

---

## üìä Uso de VRAM por Fase

### Fase 1: Generar prompts
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Llama 3.2-3B Instruct   ‚îÇ  ~2.5 GB VRAM
‚îÇ (4-bit quantization)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fase 2: Evaluar prompts
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Qwen2-Audio-7B          ‚îÇ  ~5 GB VRAM
‚îÇ (4-bit quantization)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pico m√°ximo**: ~5 GB (nunca supera 8GB) ‚úÖ

---

## ‚è±Ô∏è Tiempo por Iteraci√≥n (8GB)

| Fase | Tiempo | Descripci√≥n |
|------|--------|-------------|
| **Carga Llama** | ~2 min | Primera vez descarga de HuggingFace |
| **Genera 3 prompts** | ~3 min | LLM genera variaciones |
| **Descarga Llama** | ~10 seg | Libera VRAM |
| **Carga Qwen2-Audio** | ~2 min | Ya descargado localmente |
| **Eval√∫a 3 prompts** | ~30 min | 10 min √ó 3 prompts |
| **Descarga Qwen2-Audio** | ~10 seg | Libera VRAM |
| **Total** | **~40 min** | Por iteraci√≥n |

**Test completo (5 iteraciones)**: ~3.5 horas

---

## üñ•Ô∏è Para Servidor Remoto (Producci√≥n)

Cuando pases al servidor con m√°s VRAM (>12GB), usa el script completo:

```bash
# En servidor potente (2 modelos simult√°neos, m√°s r√°pido)
python scripts/run_opro_local.py \
    --optimizer_llm "meta-llama/Llama-3.1-8B-Instruct" \
    --n_iterations 50 \
    --early_stopping 5 \
    --output_dir results/sprint9_opro_servidor
```

**Diferencias con servidor**:
- Carga ambos modelos a la vez (no alterna)
- Llama 3.1-8B en lugar de 3.2-3B (mejor calidad)
- M√°s iteraciones (50 vs 5)
- M√°s r√°pido (~30 min por iteraci√≥n)

---

## üìÅ Workflow Completo: Laptop ‚Üí Servidor

### 1. Prototipar en Laptop (8GB) - HOY

```bash
# Test r√°pido (5 iteraciones, ~3.5 horas)
python scripts/run_opro_local_8gb.py \
    --n_iterations 5 \
    --output_dir results/sprint9_opro_laptop_test
```

**Objetivo**: Validar que todo funciona

### 2. Transferir al Servidor

```bash
# Copiar scripts y datos necesarios
scp -r scripts/ usuario@servidor:/path/to/OPRO_Qwen/
scp -r data/processed/conditions_final/ usuario@servidor:/path/to/OPRO_Qwen/data/processed/
```

### 3. Ejecutar en Servidor (>12GB)

```bash
# SSH al servidor
ssh usuario@servidor

# Optimizaci√≥n completa (30-50 iteraciones, ~20 horas)
cd /path/to/OPRO_Qwen
python scripts/run_opro_local.py \
    --optimizer_llm "meta-llama/Llama-3.1-8B-Instruct" \
    --n_iterations 50 \
    --early_stopping 5 \
    --output_dir results/sprint9_opro_servidor

# D√©jalo correr con screen o tmux
screen -S opro
python scripts/run_opro_local.py ...
# Ctrl+A D para detach
```

### 4. Descargar Resultados

```bash
# En tu laptop
scp -r usuario@servidor:/path/to/OPRO_Qwen/results/sprint9_opro_servidor/ ./results/
```

---

## üîç Monitorear Durante Ejecuci√≥n

### En laptop (otra terminal):

```bash
# Ver uso de GPU
watch -n 1 nvidia-smi

# Ver mejor prompt actual
tail -f results/sprint9_opro_laptop_test/best_prompt.txt

# Ver progreso (cuenta l√≠neas en jsonl)
wc -l results/sprint9_opro_laptop_test/opro_prompts.jsonl
# Divide por 3 para obtener iteraciones completadas
```

---

## üéì Modelos Recomendados

### Para tu Laptop (8GB):
- **Llama 3.2-3B-Instruct** ‚úÖ (por defecto, ~2.5GB)
- Qwen2.5-3B-Instruct (~2.5GB)
- Phi-3-mini-4k-instruct (~2GB)

### Para Servidor (>12GB):
- **Llama 3.1-8B-Instruct** ‚úÖ (mejor calidad, ~5GB)
- Qwen2.5-7B-Instruct (~4.5GB)
- Mistral-7B-Instruct (~4.5GB)

---

## ‚úÖ Checklist para Empezar

Antes de correr en laptop:

- [x] GPU: RTX 4070 (8GB) ‚úì
- [x] VRAM libre: ~7GB ‚úì
- [ ] Tiempo disponible: 3-4 horas
- [ ] Internet: Solo para primera descarga de Llama 3.2-3B (~1.5GB)

**Todo listo! Puedes empezar:**

```bash
python scripts/run_opro_local_8gb.py \
    --n_iterations 5 \
    --output_dir results/sprint9_opro_laptop_test
```

---

## üêõ Troubleshooting

### Error: "CUDA out of memory" durante generaci√≥n

**Causa**: Llama 3.2-3B todav√≠a muy grande

**Soluci√≥n**: Usa Phi-3-mini (m√°s peque√±o)
```bash
python scripts/run_opro_local_8gb.py \
    --optimizer_llm "microsoft/Phi-3-mini-4k-instruct" \
    --n_iterations 5
```

### Error: "CUDA out of memory" durante evaluaci√≥n

**Causa**: Qwen2-Audio no cabe (raro con 8GB)

**Soluci√≥n**: No deber√≠a pasar con 4-bit. Revisa procesos en GPU:
```bash
nvidia-smi
# Cierra otros programas que usen GPU
```

### El script se detiene entre fases

**Es NORMAL**: Est√° descargando un modelo y cargando el otro. Ver√°s:
```
Unloading optimizer LLM...
Loading Qwen2-Audio (evaluator)...
```

Esto toma ~2 minutos. **No lo interrumpas.**

---

## üìä Resultados Esperados

Con 5 iteraciones (prototipo):

| M√©trica | Baseline | Esperado | Mejora |
|---------|----------|----------|--------|
| BA_clip | 0.690 | 0.700-0.720 | +0.01 a +0.03 |

Con 50 iteraciones (servidor):

| M√©trica | Baseline | Esperado | Mejora |
|---------|----------|----------|--------|
| BA_clip | 0.690 | 0.720-0.750 | +0.03 a +0.06 |

---

## üéØ Pr√≥ximos Pasos

1. **HOY - Laptop**: Corre prototipo (5 iter, 3.5h)
   ```bash
   python scripts/run_opro_local_8gb.py --n_iterations 5
   ```

2. **Revisar**: Verifica que funciona y genera prompts razonables
   ```bash
   cat results/sprint9_opro_laptop_test/best_prompt.txt
   ```

3. **Ma√±ana - Servidor**: Transferir y correr optimizaci√≥n completa (50 iter, 20h)

4. **Despu√©s**: Refit psychometric curves + test eval

---

**¬øListo para empezar el prototipo en tu laptop?** üöÄ
