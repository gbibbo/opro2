# Trabajando con OPRO Qwen en Sistemas de Baja Memoria (8GB RAM)

## Tu Situaci√≥n

- **RAM disponible**: 7.8GB
- **GPU**: No disponible en WSL (NVML bloqueado)
- **Modelo**: Qwen2-Audio-7B (7 mil millones de par√°metros)

**Realidad**: El modelo Qwen2-Audio-7B necesita **16GB+ RAM** incluso con cuantizaci√≥n 4-bit.

---

## Opciones que S√ç Funcionan en Tu Sistema

### ‚úÖ Opci√≥n 1: Analizar Resultados Existentes (RECOMENDADO)

**NO carga el modelo** - solo lee CSVs que ya fueron generados.

```bash
python scripts/analyze_existing_results.py
```

**Output que obtienes**:
- Top 5 prompts del dev set
- Evaluaci√≥n detallada en test set
- Comparaci√≥n con baseline
- An√°lisis de errores y confianza

**Ventajas**:
- ‚úÖ Instant√°neo (< 1 segundo)
- ‚úÖ 0 MB de RAM
- ‚úÖ Muestra todos los insights importantes

**Ya lo probaste y funcion√≥!**

---

### ‚úÖ Opci√≥n 2: Usar Resultados Pre-calculados para Tu Propio An√°lisis

Los resultados ya est√°n en:

```bash
# Resultados de prompt search en dev
results/prompt_opt_local/prompt_test_results_20251022_225050.csv

# Mejor prompt encontrado
results/prompt_opt_local/best_prompt.txt

# Evaluaci√≥n del mejor prompt en test
results/prompt_opt_local/test_best_prompt_seed42.csv

# Baseline (prompt original)
checkpoints/ablations/LORA_attn_mlp/seed_42/test_predictions.csv
```

**Ejemplo de an√°lisis personalizado**:

```python
import pandas as pd
import matplotlib.pyplot as plt

# Leer resultados
df_dev = pd.read_csv('results/prompt_opt_local/prompt_test_results_20251022_225050.csv')
df_test = pd.read_csv('results/prompt_opt_local/test_best_prompt_seed42.csv')
df_baseline = pd.read_csv('checkpoints/ablations/LORA_attn_mlp/seed_42/test_predictions.csv')

# Crear gr√°fico de comparaci√≥n
fig, ax = plt.subplots(figsize=(10, 6))

methods = ['Baseline', 'Optimized Prompt']
speech_acc = [
    df_baseline[df_baseline['ground_truth'] == 'SPEECH']['correct'].mean(),
    df_test[df_test['ground_truth'] == 'SPEECH']['correct'].mean()
]
nonspeech_acc = [
    df_baseline[df_baseline['ground_truth'] == 'NONSPEECH']['correct'].mean(),
    df_test[df_test['ground_truth'] == 'NONSPEECH']['correct'].mean()
]

x = range(len(methods))
width = 0.35

ax.bar([i - width/2 for i in x], speech_acc, width, label='SPEECH')
ax.bar([i + width/2 for i in x], nonspeech_acc, width, label='NONSPEECH')

ax.set_ylabel('Accuracy')
ax.set_title('Baseline vs Optimized Prompt')
ax.set_xticks(x)
ax.set_xticklabels(methods)
ax.legend()

plt.savefig('results/comparison.png')
print("Gr√°fico guardado en results/comparison.png")
```

---

### ‚úÖ Opci√≥n 3: Crear Tu Propio Prompt y Evaluarlo (En Otro Sistema)

Si tienes acceso a una m√°quina con m√°s RAM o GPU:

**En Google Colab (GRATIS, 12GB+ RAM):**

1. Sube tu checkpoint a Google Drive
2. Monta Drive en Colab
3. Ejecuta:

```python
!pip install -q transformers peft bitsandbytes soundfile

# Tu c√≥digo de evaluaci√≥n aqu√≠
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
# ... (resto del c√≥digo)
```

**En Lambda Labs / RunPod (Pago, ~$0.50/hora):**

```bash
# SSH a la instancia
ssh user@gpu-instance

# Clonar repo
git clone <tu-repo>
cd OPRO\ Qwen

# Ejecutar
python scripts/test_prompt_templates.py \
    --checkpoint checkpoints/ablations/LORA_attn_mlp/seed_42/final \
    --test_csv data/processed/grouped_split/dev_metadata.csv \
    --output_dir results/my_results \
    --num_samples 72
```

---

## Opciones que NO Funcionan en Tu Sistema

### ‚ùå Cargar el Modelo Localmente

```bash
# Esto SIEMPRE ser√° "Killed" en tu sistema
python scripts/test_prompt_templates.py ...
python scripts/evaluate_with_logits.py ...
python scripts/finetune_qwen_audio.py ...
```

**Por qu√©**:
- Qwen2-Audio-7B base: ~14GB en FP16
- Con 4-bit quantization: ~3.5GB
- Con LoRA adapters: +500MB
- Overhead de PyTorch/Transformers: +2GB
- **M√≠nimo necesario: ~6-7GB solo para cargar**
- **Para inferencia: +2-3GB adicionales**
- **Total: ~10GB m√≠nimo**

Tu sistema: 7.8GB total ‚Üí OOM garantizado

---

## Qu√© Hacer para Ejecutar el Modelo

### Soluci√≥n A: Upgrade de Hardware (Local)

**Comprar m√°s RAM**:
- M√≠nimo: 16GB (funcional pero justo)
- Recomendado: 32GB (c√≥modo)
- Ideal: 64GB (sin problemas)

**Costo**: $30-150 USD dependiendo de tu sistema

### Soluci√≥n B: Cloud GPU (Temporal)

**Google Colab**:
- ‚úÖ GRATIS (con l√≠mites)
- ‚úÖ 12-15GB RAM
- ‚úÖ GPU T4 opcional
- ‚ùå Sesiones de 12h m√°ximo
- ‚ùå Puede desconectarse

**C√≥mo usar**:
1. Ve a [colab.research.google.com](https://colab.research.google.com)
2. Nuevo notebook
3. Runtime ‚Üí Change runtime type ‚Üí GPU
4. Sube checkpoint o conecta Drive

**Lambda Labs** ($0.50-2.00/hora):
- ‚úÖ GPU potentes (A100, H100)
- ‚úÖ RAM abundante (80GB+)
- ‚úÖ Setup completo
- ‚ùå Requiere tarjeta de cr√©dito

**C√≥mo usar**:
```bash
# Crear instancia en lambda.cloud
# SSH
ssh ubuntu@<instance-ip>

# Clonar y ejecutar
git clone <repo>
cd OPRO\ Qwen
python scripts/test_prompt_templates.py ...
```

### Soluci√≥n C: WSL con GPU (Si tienes GPU NVIDIA en Windows)

Si tu PC tiene GPU NVIDIA pero est√° bloqueada en WSL:

```bash
# En PowerShell como administrador (Windows)
wsl --update
wsl --shutdown

# Verificar drivers
nvidia-smi

# Instalar CUDA toolkit en WSL
# Seguir: https://docs.nvidia.com/cuda/wsl-user-guide/
```

Si logras habilitar GPU en WSL:
- RAM requerida baja a ~4-6GB (usa VRAM de GPU)
- Velocidad 10-50x m√°s r√°pida

---

## Resumen: Qu√© Puedes Hacer AHORA

### Con tu sistema actual (8GB RAM, sin GPU):

‚úÖ **Analizar resultados existentes**:
```bash
python scripts/analyze_existing_results.py
```

‚úÖ **Explorar CSVs manualmente**:
```bash
head -20 results/prompt_opt_local/test_best_prompt_seed42.csv
```

‚úÖ **Crear visualizaciones** (Matplotlib, Pandas):
```python
import pandas as pd
df = pd.read_csv('results/prompt_opt_local/test_best_prompt_seed42.csv')
print(df.groupby('ground_truth')['correct'].mean())
```

‚úÖ **Dise√±ar nuevos prompts** (para probar en otro sistema):
```bash
# Editar lista de prompts
nano scripts/test_prompt_templates.py

# Guardar para ejecutar en Colab/cloud
```

### Para ejecutar el modelo:

üîÑ **Opci√≥n r√°pida**: Google Colab (gratis, 1 hora setup)

üí∞ **Opci√≥n profesional**: Lambda Labs ($2-5 total para tus experimentos)

üõ†Ô∏è **Opci√≥n permanente**: Upgrade RAM a 32GB ($50-100)

---

## Ejemplo: Workflow H√≠brido Recomendado

```bash
# 1. Dise√±ar experimentos en tu sistema (8GB)
nano scripts/test_prompt_templates.py  # A√±adir tus prompts

# 2. Subir a Colab o cloud
scp -r scripts/ user@cloud:/workspace/

# 3. Ejecutar en cloud (16GB+)
ssh user@cloud
cd /workspace
python scripts/test_prompt_templates.py ...

# 4. Descargar resultados
scp user@cloud:/workspace/results/*.csv results/

# 5. Analizar en tu sistema (8GB)
python scripts/analyze_existing_results.py
python custom_analysis.py
```

**Costo**: $0 (Colab) o $1-2 (Lambda, 1-2 horas)

---

## FAQ

**P: ¬øPuedo usar CPU en vez de GPU?**
A: S√≠, pero el modelo IGUAL necesita 10GB+ RAM para cargar, y ser√° 50x m√°s lento.

**P: ¬øY si cierro todos los programas?**
A: Ayuda, pero solo liberar√°s ~500MB. Necesitas 10GB, tienes 7.8GB ‚Üí imposible.

**P: ¬øPuedo usar swap?**
A: S√≠, pero ser√° EXTREMADAMENTE lento (100-1000x). Un prompt tomar√≠a 30-60 minutos.

**P: ¬øQu√© tal 4-bit quantization?**
A: Ya est√° incluida en el script. Aun as√≠ necesitas 6-7GB solo para cargar.

**P: ¬øCu√°l es la soluci√≥n M√ÅS BARATA?**
A: Google Colab gratuito (l√≠mite 12h/d√≠a).

**P: ¬øCu√°l es la soluci√≥n M√ÅS R√ÅPIDA?**
A: Comprar RAM (si tu motherboard soporta 32GB).

---

## Conclusi√≥n

**Tu hardware actual (8GB RAM) es suficiente para**:
- ‚úÖ An√°lisis de datos
- ‚úÖ Visualizaciones
- ‚úÖ Dise√±o de experimentos
- ‚úÖ C√≥digo/debugging
- ‚úÖ Git/documentaci√≥n

**NO es suficiente para**:
- ‚ùå Cargar modelos de 7B par√°metros
- ‚ùå Fine-tuning
- ‚ùå Inferencia del modelo

**Recomendaci√≥n**: Usa Google Colab (gratis) para ejecutar el modelo, y tu sistema para todo lo dem√°s.

---

## Siguiente Paso Recomendado

Ejecuta esto en tu sistema AHORA:

```bash
python scripts/analyze_existing_results.py
```

Te mostrar√° todos los resultados de prompt optimization sin necesitar m√°s RAM.

Luego, si quieres probar tus propios prompts:

1. Ed√≠talos en `scripts/test_prompt_templates.py`
2. Sube el repo a Google Colab
3. Ejecuta all√≠
4. Descarga resultados
5. Analiza en tu sistema

**Tiempo total**: ~30 minutos
**Costo**: $0
